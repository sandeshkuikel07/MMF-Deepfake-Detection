# Model Architecture Guide: Comprehensive Deepfake Detection System

## Overview

This document provides a detailed explanation of all **9 models** implemented in the comprehensive deepfake detection notebook. These models are organized into three categories, enabling systematic comparison of different approaches and attention mechanisms.

---

## ðŸ“Š Category 1: Individual Domain Models (Baseline)

These models use features from a single domain without any attention mechanisms, serving as baseline comparisons.

### 1. Spatial Baseline (`spatial_baseline`)

**Input Features:**
- Xception CNN features: **2048 dimensions**
- Extracted from face images at 299Ã—299 resolution
- Pre-trained on ImageNet, fine-tuned for texture analysis

**Architecture:**
```
Input (2048) â†’ Dense(512) â†’ ReLU â†’ Dropout(0.5)
           â†’ Dense(256) â†’ ReLU â†’ Dropout(0.5)
           â†’ Dense(1) â†’ Sigmoid
```

**Detection Strategy:**
- Analyzes texture-level artifacts and pixel inconsistencies
- Detects blending artifacts, compression artifacts, and unnatural textures
- Focuses on local spatial patterns

**Strengths:**
- âœ… Excellent at capturing fine-grained spatial anomalies
- âœ… Proven effectiveness on GAN-generated faces
- âœ… High-dimensional rich feature representation

**Limitations:**
- âŒ Cannot leverage frequency domain information
- âŒ Misses periodic patterns and global statistics
- âŒ Limited to spatial artifacts only

**Expected Performance:** Moderate baseline (~70-80% accuracy on FaceForensics++)

---

### 2. Frequency Baseline (`frequency_baseline`)

**Input Features:**
- FFT (Fast Fourier Transform) statistical features: **4 dimensions**
  1. Mean of frequency magnitude spectrum
  2. Variance of frequency magnitude spectrum
  3. Skewness of frequency magnitude spectrum
  4. Kurtosis of frequency magnitude spectrum

**Architecture:**
```
Input (4) â†’ Dense(512) â†’ ReLU â†’ Dropout(0.5)
         â†’ Dense(256) â†’ ReLU â†’ Dropout(0.5)
         â†’ Dense(1) â†’ Sigmoid
```

**Detection Strategy:**
- Analyzes frequency domain anomalies
- Detects periodic patterns introduced by GAN upsampling
- Identifies compression artifacts in frequency space

**Strengths:**
- âœ… Captures artifacts invisible in spatial domain
- âœ… Robust to certain image manipulations
- âœ… Lightweight and fast inference

**Limitations:**
- âŒ Very limited feature dimensionality (only 4 features)
- âŒ Misses spatial context and texture details
- âŒ May struggle with diverse manipulation methods

**Expected Performance:** Lower baseline (~60-70% accuracy) due to limited features

---

### 3. Semantic Baseline (`semantic_baseline`)

**Input Features:**
- DINOv2 ViT-B/14 features: **768 dimensions**
- Self-supervised vision transformer trained on large-scale data
- Captures high-level semantic representations

**Architecture:**
```
Input (768) â†’ Dense(512) â†’ ReLU â†’ Dropout(0.5)
          â†’ Dense(256) â†’ ReLU â†’ Dropout(0.5)
          â†’ Dense(1) â†’ Sigmoid
```

**Detection Strategy:**
- Detects high-level semantic inconsistencies
- Identifies unnatural facial compositions
- Captures context and relationship between facial features

**Strengths:**
- âœ… Strong semantic understanding from self-supervised learning
- âœ… Captures contextual and compositional forgeries
- âœ… Generalizes well across different manipulation types

**Limitations:**
- âŒ May miss low-level pixel artifacts
- âŒ Less sensitive to subtle texture inconsistencies
- âŒ Cannot detect frequency-specific anomalies

**Expected Performance:** Moderate-to-good baseline (~75-85% accuracy)

---

## ðŸŽ¯ Category 2: Individual Domain Models (With Attention)

These models enhance single-domain classifiers with domain-specific attention mechanisms, learning to focus on the most discriminative features.

### 4. Spatial + Channel Attention (`spatial_attention`)

**Input Features:**
- Xception features: **2048 dimensions**

**Attention Mechanism:**
- **Squeeze-and-Excitation (SE) Channel Attention**
- Learns channel-wise importance weights
- Architecture:
  ```
  Input (2048) â†’ Global Pool â†’ FC(128) â†’ ReLU
              â†’ FC(2048) â†’ Sigmoid â†’ Channel Weights
  Attended Features = Input Ã— Channel Weights
  ```

**How It Works:**
1. Squeeze: Compute global statistics for each channel
2. Excitation: Learn channel importance via 2-layer MLP
3. Recalibration: Scale channels by learned weights

**Benefits:**
- âœ… Focuses on most relevant spatial features
- âœ… Reduces noise from uninformative channels
- âœ… Adaptive feature selection per input

**Expected Improvement:** +2-5% accuracy over spatial baseline

---

### 5. Frequency + Band Attention (`frequency_attention`)

**Input Features:**
- FFT statistical features: **4 dimensions**

**Attention Mechanism:**
- **Frequency Band-Specific Attention**
- Learns importance weights for each statistic (mean, var, skew, kurtosis)
- Architecture:
  ```
  Input (4) â†’ FC(16) â†’ ReLU â†’ FC(4) â†’ Sigmoid â†’ Band Weights
  Attended Features = Input Ã— Band Weights
  ```

**How It Works:**
1. Learns which frequency statistics are most discriminative
2. Dynamically weights mean, variance, skewness, and kurtosis
3. Adapts to different manipulation methods

**Benefits:**
- âœ… Emphasizes most informative frequency characteristics
- âœ… Reduces impact of less relevant statistics
- âœ… Improves limited feature representation

**Expected Improvement:** +1-3% accuracy over frequency baseline

---

### 6. Semantic + Self-Attention (`semantic_attention`)

**Input Features:**
- DINOv2 features: **768 dimensions**

**Attention Mechanism:**
- **Multi-Head Self-Attention**
- Captures relationships within the semantic feature vector
- 8 attention heads, each with 96-dimensional subspace
- Architecture:
  ```
  Input (768) â†’ Q, K, V projections (768 Ã— 3)
             â†’ Multi-head Attention (8 heads)
             â†’ Output Projection (768)
             â†’ Layer Norm + Residual Connection
  ```

**How It Works:**
1. Projects features into Query, Key, Value representations
2. Computes attention between feature dimensions
3. Refines features based on internal relationships

**Benefits:**
- âœ… Better feature refinement through self-attention
- âœ… Captures complex feature interactions
- âœ… Enhanced semantic context modeling

**Expected Improvement:** +2-4% accuracy over semantic baseline

---

## ðŸ”— Category 3: Multi-Domain Fusion Models

These models combine all three domains (spatial + frequency + semantic) for comprehensive detection, leveraging complementary information.

### 7. Baseline Fusion (`baseline_fusion`)

**Input Features:**
- Spatial (2048) + Frequency (4) + Semantic (768) = **2820 dimensions**

**Fusion Strategy:**
- **Simple Concatenation**
- All features concatenated into single vector
- No learned weighting or attention

**Architecture:**
```
Concat[Spatial, Frequency, Semantic] (2820)
  â†’ Dense(512) â†’ ReLU â†’ Dropout(0.5)
  â†’ Dense(256) â†’ ReLU â†’ Dropout(0.5)
  â†’ Dense(1) â†’ Sigmoid
```

**How It Works:**
1. Concatenate all domain features
2. Process through standard MLP classifier
3. Equal, static weighting of all domains

**Strengths:**
- âœ… Leverages complementary information from all domains
- âœ… Simple and straightforward approach
- âœ… Strong baseline for multi-domain detection

**Limitations:**
- âŒ Cannot adapt weights based on input characteristics
- âŒ Equal weighting may be suboptimal
- âŒ No mechanism to suppress noisy domains

**Expected Performance:** ~80-90% accuracy (significant improvement over single domains)

---

### 8. Attention Fusion (`attention_fusion`)

**Input Features:**
- Spatial (2048), Frequency (4), Semantic (768)
- **No individual attention applied** (raw features used)

**Fusion Strategy:**
- **Learnable Domain Attention Fusion**

**Architecture:**
```
Domain Projection:
  Spatial (2048) â†’ FC(512)
  Frequency (4) â†’ FC(512)
  Semantic (768) â†’ FC(512)

Domain Attention:
  Concat[Spatial_proj, Freq_proj, Semantic_proj] (1536)
    â†’ FC(256) â†’ ReLU â†’ Dropout(0.3)
    â†’ FC(3) â†’ Softmax â†’ Domain Weights [w1, w2, w3]

Fusion:
  Weighted_Sum = w1Ã—Spatial_proj + w2Ã—Freq_proj + w3Ã—Semantic_proj
    â†’ FC(512) â†’ ReLU â†’ Dropout(0.3)
    â†’ FC(256) â†’ ReLU â†’ Dropout(0.3)
    â†’ FC(1) â†’ Sigmoid
```

**How It Works:**
1. **Project** each domain to common 512-dim space
2. **Compute attention weights** via MLP over concatenated projections
3. **Weight and sum** domain features dynamically per sample
4. **Classify** from fused representation

**Key Innovation:**
- ðŸ”‘ Model learns which domain is most reliable for each input
- ðŸ”‘ Attention weights adapt based on manipulation type
- ðŸ”‘ Can suppress unreliable domains dynamically

**Benefits:**
- âœ… Adaptive domain weighting per sample
- âœ… Handles varying domain reliability
- âœ… More expressive than simple concatenation

**Expected Improvement:** +3-8% accuracy over baseline fusion

**Example Attention Patterns:**
- Face2Face (expression manipulation) â†’ Higher semantic weight
- Deepfakes (face swap) â†’ Higher spatial weight
- Neural Textures (texture synthesis) â†’ Higher frequency weight

---

### 9. Complete Attention (`complete_attention`) â­ **Most Advanced**

**Input Features:**
- Spatial (2048), Frequency (4), Semantic (768)

**Fusion Strategy:**
- **Two-Stage Attention Pipeline**
  1. Individual domain attention (refine features)
  2. Cross-domain attention fusion (combine features)

**Architecture:**

**Stage 1 - Individual Domain Attention:**
```
Spatial Branch:
  Spatial (2048) â†’ Channel Attention â†’ Attended_Spatial (2048)

Frequency Branch:
  Frequency (4) â†’ Band Attention â†’ Attended_Frequency (4)

Semantic Branch:
  Semantic (768) â†’ Self-Attention â†’ Attended_Semantic (768)
```

**Stage 2 - Fusion Attention:**
```
Domain Projection:
  Attended_Spatial (2048) â†’ FC(512)
  Attended_Frequency (4) â†’ FC(512)
  Attended_Semantic (768) â†’ FC(512)

Domain Attention Fusion:
  [Same as Attention Fusion model]
  â†’ Dynamic weighting â†’ Fused (512)

Classification:
  Fused (512) â†’ FC(256) â†’ ReLU â†’ Dropout(0.5)
             â†’ FC(1) â†’ Sigmoid
```

**How It Works:**
1. **Within-Domain Refinement**: Each domain applies its specific attention
   - Spatial: Channel attention selects important texture features
   - Frequency: Band attention weights frequency statistics
   - Semantic: Self-attention refines semantic relationships
2. **Cross-Domain Fusion**: Learned attention weights combine refined features
3. **Classification**: Final MLP processes fused representation

**Key Innovation:**
- ðŸ”‘ **Hierarchical attention**: Both within-domain and cross-domain
- ðŸ”‘ **Maximum expressiveness**: Can refine features at multiple levels
- ðŸ”‘ **Complementary mechanisms**: Different attention types for different domains

**Benefits:**
- âœ… Best feature refinement at individual level
- âœ… Best combination strategy at fusion level
- âœ… Most flexible and adaptive architecture
- âœ… Captures both fine-grained and global patterns

**Expected Improvement:**
- +5-12% accuracy over baseline fusion
- +2-5% accuracy over attention fusion
- **Hypothesis**: Should be the best performing model

**Why It Should Work Best:**
- Refines noisy features before fusion
- Adapts at both individual and fusion levels
- Leverages complementary strengths of all attention types
- Most parameters and highest capacity

---

## ðŸŽ“ Comparison Strategy

The 9 models enable **three key types of comparisons**:

### 1. Within-Domain Attention Effect
**Comparison:** Baseline vs Attention for each domain (Models 1-6)

| Domain | Baseline | With Attention | Purpose |
|--------|----------|----------------|---------|
| Spatial | Model 1 | Model 4 | Quantify SE attention benefit |
| Frequency | Model 2 | Model 5 | Quantify band attention benefit |
| Semantic | Model 3 | Model 6 | Quantify self-attention benefit |

**Question Answered:** *How much does attention improve single-domain detection?*

---

### 2. Cross-Domain Benefit
**Comparison:** Single domains vs Multi-domain fusion (Models 1-3 vs 7-9)

| Approach | Models | Purpose |
|----------|--------|---------|
| Single Domain | 1, 2, 3 | Individual domain capabilities |
| Multi-Domain | 7, 8, 9 | Combined domain performance |

**Question Answered:** *What is the value of combining complementary information?*

**Expected Finding:** Multi-domain should significantly outperform any single domain, demonstrating that different domains capture orthogonal information.

---

### 3. Attention Architecture Impact
**Comparison:** Baseline â†’ Fusion Attention â†’ Complete Attention (Models 7-9)

| Model | Attention Level | Expected Rank |
|-------|----------------|---------------|
| Baseline Fusion | None | 3rd |
| Attention Fusion | Fusion only | 2nd |
| Complete Attention | Individual + Fusion | 1st |

**Question Answered:** *Where should attention be applied for maximum benefit?*

**Hypothesis:** Complete attention (both levels) > Fusion attention only > No attention

---

## ðŸ“ˆ Expected Performance Ranking

### Predicted Order (Best to Worst):

| Rank | Model | Attention Type | Expected Accuracy |
|------|-------|----------------|-------------------|
| ðŸ¥‡ 1st | Complete Attention | Individual + Fusion | **90-95%** |
| ðŸ¥ˆ 2nd | Attention Fusion | Fusion only | **87-92%** |
| ðŸ¥‰ 3rd | Baseline Fusion | None | **82-88%** |
| 4th | Semantic Attention | Self-attention | **78-85%** |
| 5th | Spatial Attention | Channel attention | **75-82%** |
| 6th | Frequency Attention | Band attention | **63-72%** |
| 7th | Semantic Baseline | None | **75-83%** |
| 8th | Spatial Baseline | None | **72-80%** |
| 9th | Frequency Baseline | None | **60-70%** |

### Key Hypotheses:

1. **Multi-domain > Single-domain**: Fusion models (7-9) should outperform all individual domains (1-6)
2. **Attention helps**: All attention models should beat their baseline counterparts
3. **Complete > Partial**: Complete attention should outperform attention fusion
4. **Semantic strongest single**: Semantic features should be the best single domain
5. **Frequency weakest**: Limited 4-dim features make frequency the weakest alone

---

## ðŸ§ª Ablation Study Insights

The model architecture enables systematic ablation:

### Ablation 1: Attention Contribution (Per Domain)
```
Improvement = Accuracy(Domain + Attention) - Accuracy(Domain Baseline)
```
**Expected**: Spatial â‰ˆ Semantic > Frequency (due to feature dimensionality)

### Ablation 2: Multi-Domain Value
```
Improvement = Accuracy(Baseline Fusion) - Max(Single Domain Accuracies)
```
**Expected**: +5-10% improvement from combining domains

### Ablation 3: Fusion Attention Value
```
Improvement = Accuracy(Attention Fusion) - Accuracy(Baseline Fusion)
```
**Expected**: +3-8% improvement from learnable fusion

### Ablation 4: Complete Attention Value
```
Improvement = Accuracy(Complete Attention) - Accuracy(Attention Fusion)
```
**Expected**: +2-5% improvement from individual domain attention

---

## ðŸ“Š Attention Weight Analysis

### Domain Attention Weights (Models 8-9)

**Expected Patterns:**
- **Spatial weight**: Higher for face-swap methods (Deepfakes, FaceSwap)
- **Frequency weight**: Higher for GAN-based methods (Neural Textures)
- **Semantic weight**: Higher for expression/attribute manipulation (Face2Face)

**Real vs Fake Differences:**
- Real faces: More balanced weights (all domains agree)
- Fake faces: More extreme weights (model relies on specific domain)

### Channel Attention Patterns (Model 4, 9)
- High weights for texture-sensitive channels
- Low weights for background/smooth region channels

### Frequency Band Weights (Model 5, 9)
- Expected: Variance and Kurtosis more important than Mean
- Different patterns for different manipulation methods

---

## ðŸŽ¯ Key Takeaways

1. **Attention Improves Performance**: All attention models should outperform baselines
2. **Multi-Domain is Essential**: Fusion models should significantly outperform single domains
3. **Hierarchical Attention Wins**: Complete attention should be the best architecture
4. **Different Domains for Different Fakes**: Attention weights reveal which domain is most discriminative
5. **Complementary Information**: Spatial, frequency, and semantic capture different artifacts

---

## ðŸ“š Technical Details

### Training Configuration
- **Optimizer**: Adam (lr=1e-4)
- **Loss**: Binary Cross-Entropy with Logits
- **Batch Size**: 128
- **Epochs**: 15 (with early stopping)
- **Regularization**: Dropout (0.3-0.5), Early stopping (patience=5)
- **Hardware**: CUDA-enabled GPU

### Dataset
- **Source**: FaceForensics++
- **Manipulation Methods**: Deepfakes, Face2Face, FaceSwap, NeuralTextures
- **Preprocessing**: MTCNN face detection â†’ 299Ã—299 resize
- **Split**: 80% train, 20% validation

### Evaluation Metrics
- **Accuracy**: Overall classification accuracy
- **Precision**: Fake detection precision
- **Recall**: Fake detection recall (true positive rate)
- **F1 Score**: Harmonic mean of precision and recall

---

## ðŸ”„ Model Selection Guide

**Choose Based On Your Needs:**

- **Maximum Accuracy**: Use **Complete Attention** (Model 9)
- **Interpretability**: Use **Attention Fusion** (Model 8) - simpler attention weights
- **Speed/Efficiency**: Use **Spatial Baseline** (Model 1) - single domain, no attention
- **Limited Data**: Use **Semantic Baseline** (Model 3) - pre-trained features generalize well
- **Specific Manipulation**: Check attention weights to identify best domain

---

## ðŸ“– References

### Feature Extractors
- **Xception**: Chollet, F. (2017). "Xception: Deep Learning with Depthwise Separable Convolutions"
- **DINOv2**: Oquab, M., et al. (2024). "DINOv2: Learning Robust Visual Features without Supervision"
- **FFT**: Classic signal processing technique for frequency domain analysis

### Attention Mechanisms
- **SE (Channel Attention)**: Hu, J., et al. (2018). "Squeeze-and-Excitation Networks"
- **Self-Attention**: Vaswani, A., et al. (2017). "Attention is All You Need"

### Dataset
- **FaceForensics++**: RÃ¶ssler, A., et al. (2019). "FaceForensics++: Learning to Detect Manipulated Facial Images"

---

**Document Version**: 1.0  
**Last Updated**: January 18, 2026  
**Associated Notebook**: `comprehensive_deepfake_detection.ipynb`
