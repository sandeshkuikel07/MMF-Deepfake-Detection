{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "815261c1",
   "metadata": {},
   "source": [
    "# Integrated Multi-Dataset Deepfake Detection Training\n",
    "\n",
    "## Overview\n",
    "This notebook implements **integrated training** across multiple deepfake detection datasets using the **Complete Attention Model**.\n",
    "\n",
    "### Datasets Integrated:\n",
    "1. **FaceForensics++**: Multiple manipulation methods (Deepfakes, Face2Face, FaceSwap, etc.)\n",
    "2. **Celeb-DF**: Celebrity deepfake dataset with high-quality fakes\n",
    "3. **Deepfake Detection Challenge (DFDC)**: Kaggle competition dataset\n",
    "4. **HiDF**: High-fidelity deepfake dataset (videos and images)\n",
    "\n",
    "### Key Features:\n",
    "- Configurable data percentage (default: 5%)\n",
    "- Complete Attention Model architecture\n",
    "- Multi-domain feature extraction (Spatial, Frequency, Semantic)\n",
    "- Cross-dataset performance analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805fe37f",
   "metadata": {},
   "source": [
    "## Section 1: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6039877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Models\n",
    "import timm\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "# Analysis and visualization\n",
    "from scipy.stats import skew, kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"{'='*70}\")\n",
    "print(\"ENVIRONMENT SETUP\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠ Running on CPU - training will be slower\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90730037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Central configuration for the integrated training pipeline.\"\"\"\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # DATA PERCENTAGE - Controls how much data to use from each dataset\n",
    "    # ==========================================================================\n",
    "    DATA_PERCENTAGE = 0.05  # 5% of each dataset (adjustable: 0.01 to 1.0)\n",
    "    \n",
    "    # ==========================================================================\n",
    "    # Dataset Paths\n",
    "    # ==========================================================================\n",
    "    BASE_DIR = Path('E:/MMF Deepfake Detection')\n",
    "    DATASETS_DIR = BASE_DIR / 'datasets'\n",
    "    \n",
    "    # Individual dataset paths\n",
    "    FFPP_DIR = DATASETS_DIR / 'FaceForensics++'\n",
    "    CELEBDF_DIR = DATASETS_DIR / 'CelebDF'\n",
    "    DFDC_DIR = DATASETS_DIR / 'deepfake-detection-challenge'\n",
    "    HIDF_DIR = DATASETS_DIR / 'HiDF'\n",
    "    \n",
    "    # Output directories\n",
    "    PREPROCESSED_DIR = BASE_DIR / 'preprocessed_faces_integrated'\n",
    "    FEATURES_DIR = BASE_DIR / 'extracted_features_integrated'\n",
    "    CHECKPOINTS_DIR = BASE_DIR / 'checkpoints'\n",
    "    RESULTS_DIR = BASE_DIR / 'results'\n",
    "    INTEGRATED_METADATA_PATH = BASE_DIR / 'integrated_metadata.csv'\n",
    "    \n",
    "    # Data settings\n",
    "    FRAMES_PER_VIDEO = 20\n",
    "    FACE_SIZE = 299\n",
    "    TRAIN_RATIO = 0.8\n",
    "    \n",
    "    # Feature dimensions\n",
    "    SPATIAL_DIM = 2048   # Xception output\n",
    "    FREQ_DIM = 4         # FFT statistical features\n",
    "    SEMANTIC_DIM = 768   # DINOv2 ViT-B/14 output\n",
    "    \n",
    "    # Training settings\n",
    "    BATCH_SIZE = 64\n",
    "    LEARNING_RATE = 1e-4\n",
    "    EPOCHS = 20\n",
    "    HIDDEN_DIM1 = 512\n",
    "    HIDDEN_DIM2 = 256\n",
    "    DROPOUT = 0.5\n",
    "    FUSION_DIM = 512\n",
    "    \n",
    "    # Early stopping\n",
    "    PATIENCE = 5\n",
    "    MIN_DELTA = 0.001\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directory structure\n",
    "def create_directories():\n",
    "    \"\"\"Create all necessary directories.\"\"\"\n",
    "    dirs = [\n",
    "        config.PREPROCESSED_DIR / 'real',\n",
    "        config.PREPROCESSED_DIR / 'fake',\n",
    "        config.FEATURES_DIR / 'spatial',\n",
    "        config.FEATURES_DIR / 'frequency',\n",
    "        config.FEATURES_DIR / 'semantic',\n",
    "        config.CHECKPOINTS_DIR,\n",
    "        config.RESULTS_DIR\n",
    "    ]\n",
    "    for dir_path in dirs:\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"✓ Directory structure created\")\n",
    "\n",
    "create_directories()\n",
    "\n",
    "# Display configuration\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PROJECT CONFIGURATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Data Percentage: {config.DATA_PERCENTAGE * 100:.1f}%\")\n",
    "print(f\"Base Directory: {config.BASE_DIR.absolute()}\")\n",
    "print(f\"Frames per video: {config.FRAMES_PER_VIDEO}\")\n",
    "print(f\"Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"Epochs: {config.EPOCHS}\")\n",
    "print(f\"Feature dimensions: Spatial={config.SPATIAL_DIM}, Freq={config.FREQ_DIM}, Semantic={config.SEMANTIC_DIM}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b5907",
   "metadata": {},
   "source": [
    "## Section 2: Dataset Loaders\n",
    "\n",
    "Load and parse metadata from all four datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15134346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FACEFORENSICS++ DATASET LOADER\n",
    "# ============================================================================\n",
    "\n",
    "def load_ffpp_dataset(data_percentage=None):\n",
    "    \"\"\"\n",
    "    Load FaceForensics++ dataset metadata.\n",
    "    Uses CSV files from datasets/FaceForensics++/csv/\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: video_id, label, path, manipulation, dataset\n",
    "    \"\"\"\n",
    "    data_percentage = data_percentage or config.DATA_PERCENTAGE\n",
    "    csv_dir = config.FFPP_DIR / 'csv'\n",
    "    \n",
    "    if not csv_dir.exists():\n",
    "        print(f\"⚠ FaceForensics++ CSV directory not found: {csv_dir}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    # Load original (real) videos\n",
    "    original_csv = csv_dir / 'original.csv'\n",
    "    if original_csv.exists():\n",
    "        df_orig = pd.read_csv(original_csv)\n",
    "        # Sample the required percentage\n",
    "        n_samples = max(1, int(len(df_orig) * data_percentage))\n",
    "        df_orig_sampled = df_orig.sample(n=n_samples, random_state=42)\n",
    "        \n",
    "        for _, row in df_orig_sampled.iterrows():\n",
    "            file_path = row['File Path']\n",
    "            video_name = Path(file_path).stem\n",
    "            full_path = config.FFPP_DIR / file_path\n",
    "            if full_path.exists():\n",
    "                data.append({\n",
    "                    'video_id': f'FFPP_original_{video_name}',\n",
    "                    'label': 'real',\n",
    "                    'path': str(full_path),\n",
    "                    'manipulation': 'Original',\n",
    "                    'dataset': 'FaceForensics++'\n",
    "                })\n",
    "    \n",
    "    # Load fake videos from different manipulation methods\n",
    "    manipulation_csvs = ['Deepfakes.csv', 'Face2Face.csv', 'FaceSwap.csv', \n",
    "                         'NeuralTextures.csv', 'FaceShifter.csv']\n",
    "    \n",
    "    for csv_name in manipulation_csvs:\n",
    "        csv_path = csv_dir / csv_name\n",
    "        if csv_path.exists():\n",
    "            df = pd.read_csv(csv_path)\n",
    "            manipulation = csv_name.replace('.csv', '')\n",
    "            \n",
    "            # Sample the required percentage\n",
    "            n_samples = max(1, int(len(df) * data_percentage))\n",
    "            df_sampled = df.sample(n=n_samples, random_state=42)\n",
    "            \n",
    "            for _, row in df_sampled.iterrows():\n",
    "                file_path = row['File Path']\n",
    "                video_name = Path(file_path).stem\n",
    "                full_path = config.FFPP_DIR / file_path\n",
    "                if full_path.exists():\n",
    "                    data.append({\n",
    "                        'video_id': f'FFPP_{manipulation}_{video_name}',\n",
    "                        'label': 'fake',\n",
    "                        'path': str(full_path),\n",
    "                        'manipulation': manipulation,\n",
    "                        'dataset': 'FaceForensics++'\n",
    "                    })\n",
    "    \n",
    "    # Also check FF++_Metadata.csv for DeepFakeDetection videos\n",
    "    metadata_csv = csv_dir / 'FF++_Metadata.csv'\n",
    "    if metadata_csv.exists():\n",
    "        df_meta = pd.read_csv(metadata_csv)\n",
    "        # Filter for DeepFakeDetection\n",
    "        df_dfd = df_meta[df_meta['File Path'].str.contains('DeepFakeDetection', na=False)]\n",
    "        \n",
    "        n_samples = max(1, int(len(df_dfd) * data_percentage))\n",
    "        df_dfd_sampled = df_dfd.sample(n=min(n_samples, len(df_dfd)), random_state=42)\n",
    "        \n",
    "        for _, row in df_dfd_sampled.iterrows():\n",
    "            file_path = row['File Path']\n",
    "            video_name = Path(file_path).stem\n",
    "            full_path = config.FFPP_DIR / file_path\n",
    "            label = 'fake' if row['Label'] == 'FAKE' else 'real'\n",
    "            if full_path.exists():\n",
    "                data.append({\n",
    "                    'video_id': f'FFPP_DeepFakeDetection_{video_name}',\n",
    "                    'label': label,\n",
    "                    'path': str(full_path),\n",
    "                    'manipulation': 'DeepFakeDetection',\n",
    "                    'dataset': 'FaceForensics++'\n",
    "                })\n",
    "    \n",
    "    df_result = pd.DataFrame(data)\n",
    "    print(f\"✓ FaceForensics++: Loaded {len(df_result)} videos ({data_percentage*100:.1f}%)\")\n",
    "    if len(df_result) > 0:\n",
    "        print(f\"  Real: {len(df_result[df_result['label'] == 'real'])}, Fake: {len(df_result[df_result['label'] == 'fake'])}\")\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELEB-DF DATASET LOADER\n",
    "# ============================================================================\n",
    "\n",
    "def load_celebdf_dataset(data_percentage=None):\n",
    "    \"\"\"\n",
    "    Load Celeb-DF dataset metadata.\n",
    "    Uses List_of_testing_videos.txt for ground truth labels.\n",
    "    \n",
    "    Format: <label> <path>\n",
    "    - Label 1 = Real\n",
    "    - Label 0 = Fake (Celeb-synthesis)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: video_id, label, path, manipulation, dataset\n",
    "    \"\"\"\n",
    "    data_percentage = data_percentage or config.DATA_PERCENTAGE\n",
    "    list_file = config.CELEBDF_DIR / 'List_of_testing_videos.txt'\n",
    "    \n",
    "    if not list_file.exists():\n",
    "        print(f\"⚠ Celeb-DF list file not found: {list_file}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    with open(list_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Parse all entries\n",
    "    all_entries = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 2:\n",
    "            file_label = int(parts[0])\n",
    "            file_path = parts[1]\n",
    "            all_entries.append((file_label, file_path))\n",
    "    \n",
    "    # Sample the required percentage\n",
    "    n_samples = max(1, int(len(all_entries) * data_percentage))\n",
    "    random.seed(42)\n",
    "    sampled_entries = random.sample(all_entries, min(n_samples, len(all_entries)))\n",
    "    \n",
    "    for file_label, file_path in sampled_entries:\n",
    "        video_name = Path(file_path).stem\n",
    "        folder_name = Path(file_path).parent.name\n",
    "        full_path = config.CELEBDF_DIR / file_path\n",
    "        \n",
    "        # Label 1 = Real, Label 0 = Fake\n",
    "        label = 'real' if file_label == 1 else 'fake'\n",
    "        \n",
    "        # Determine manipulation type based on folder\n",
    "        if 'synthesis' in folder_name.lower():\n",
    "            manipulation = 'Celeb-synthesis'\n",
    "        elif 'real' in folder_name.lower():\n",
    "            manipulation = 'Original'\n",
    "        else:\n",
    "            manipulation = folder_name\n",
    "        \n",
    "        if full_path.exists():\n",
    "            data.append({\n",
    "                'video_id': f'CelebDF_{folder_name}_{video_name}',\n",
    "                'label': label,\n",
    "                'path': str(full_path),\n",
    "                'manipulation': manipulation,\n",
    "                'dataset': 'Celeb-DF'\n",
    "            })\n",
    "    \n",
    "    df_result = pd.DataFrame(data)\n",
    "    print(f\"✓ Celeb-DF: Loaded {len(df_result)} videos ({data_percentage*100:.1f}%)\")\n",
    "    if len(df_result) > 0:\n",
    "        print(f\"  Real: {len(df_result[df_result['label'] == 'real'])}, Fake: {len(df_result[df_result['label'] == 'fake'])}\")\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e9ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DFDC (DEEPFAKE DETECTION CHALLENGE) DATASET LOADER\n",
    "# ============================================================================\n",
    "\n",
    "def load_dfdc_dataset(data_percentage=None):\n",
    "    \"\"\"\n",
    "    Load DFDC dataset metadata.\n",
    "    Uses metadata.json from train_sample_videos directory.\n",
    "    \n",
    "    Format: {\"filename.mp4\": {\"label\": \"REAL\"/\"FAKE\", \"split\": \"train\", \"original\": ...}}\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: video_id, label, path, manipulation, dataset\n",
    "    \"\"\"\n",
    "    data_percentage = data_percentage or config.DATA_PERCENTAGE\n",
    "    videos_dir = config.DFDC_DIR / 'train_sample_videos'\n",
    "    metadata_file = videos_dir / 'metadata.json'\n",
    "    \n",
    "    if not metadata_file.exists():\n",
    "        print(f\"⚠ DFDC metadata file not found: {metadata_file}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Get all entries\n",
    "    all_entries = list(metadata.items())\n",
    "    \n",
    "    # Sample the required percentage\n",
    "    n_samples = max(1, int(len(all_entries) * data_percentage))\n",
    "    random.seed(42)\n",
    "    sampled_entries = random.sample(all_entries, min(n_samples, len(all_entries)))\n",
    "    \n",
    "    for filename, info in sampled_entries:\n",
    "        video_path = videos_dir / filename\n",
    "        video_name = Path(filename).stem\n",
    "        label = 'real' if info['label'] == 'REAL' else 'fake'\n",
    "        \n",
    "        # Determine manipulation type\n",
    "        if label == 'fake':\n",
    "            manipulation = 'DFDC_DeepFake'\n",
    "        else:\n",
    "            manipulation = 'Original'\n",
    "        \n",
    "        if video_path.exists():\n",
    "            data.append({\n",
    "                'video_id': f'DFDC_{video_name}',\n",
    "                'label': label,\n",
    "                'path': str(video_path),\n",
    "                'manipulation': manipulation,\n",
    "                'dataset': 'DFDC'\n",
    "            })\n",
    "    \n",
    "    df_result = pd.DataFrame(data)\n",
    "    print(f\"✓ DFDC: Loaded {len(df_result)} videos ({data_percentage*100:.1f}%)\")\n",
    "    if len(df_result) > 0:\n",
    "        print(f\"  Real: {len(df_result[df_result['label'] == 'real'])}, Fake: {len(df_result[df_result['label'] == 'fake'])}\")\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ffd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HiDF DATASET LOADER\n",
    "# ============================================================================\n",
    "\n",
    "def load_hidf_dataset(data_percentage=None):\n",
    "    \"\"\"\n",
    "    Load HiDF dataset metadata using FOLDER-BASED LABELING.\n",
    "    \n",
    "    Labeling based on folder names:\n",
    "    - Files in Fake-vid, Fake-img folders → label = \"fake\"\n",
    "    - Files in Real-vid, Real-img folders → label = \"real\"\n",
    "    \n",
    "    This approach directly scans the folders instead of relying on metadata.csv,\n",
    "    which ensures accurate labeling based on actual file locations.\n",
    "    \n",
    "    Directories:\n",
    "    - Real-vid: Real videos (.mp4)\n",
    "    - Real-img: Real images (.jpg, .png) - may not exist yet\n",
    "    - Fake-vid: Fake videos (.mp4)\n",
    "    - Fake-img: Fake images (.jpg, .png)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: video_id, label, path, manipulation, dataset, media_type\n",
    "    \"\"\"\n",
    "    data_percentage = data_percentage or config.DATA_PERCENTAGE\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    # Define folder configurations: (folder_name, label, media_type, manipulation)\n",
    "    folder_configs = [\n",
    "        ('Fake-vid', 'fake', 'vid', 'HiDF_FaceSwap'),\n",
    "        ('Fake-img', 'fake', 'img', 'HiDF_FaceSwap'),\n",
    "        ('Real-vid', 'real', 'vid', 'Original'),\n",
    "        ('Real-img', 'real', 'img', 'Original'),\n",
    "    ]\n",
    "    \n",
    "    for folder_name, label, media_type, manipulation in folder_configs:\n",
    "        folder_path = config.HIDF_DIR / folder_name\n",
    "        \n",
    "        if not folder_path.exists():\n",
    "            print(f\"  ⚠ HiDF folder not found: {folder_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Define file extensions based on media type\n",
    "        if media_type == 'vid':\n",
    "            extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv']\n",
    "        else:\n",
    "            extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']\n",
    "        \n",
    "        # Collect all files from this folder\n",
    "        all_files = []\n",
    "        for ext in extensions:\n",
    "            all_files.extend(list(folder_path.glob(ext)))\n",
    "        \n",
    "        if not all_files:\n",
    "            print(f\"  ⚠ No files found in {folder_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Sample the required percentage\n",
    "        n_samples = max(1, int(len(all_files) * data_percentage))\n",
    "        random.seed(42)\n",
    "        sampled_files = random.sample(all_files, min(n_samples, len(all_files)))\n",
    "        \n",
    "        # Add sampled files to data\n",
    "        for file_path in sampled_files:\n",
    "            file_name = file_path.stem\n",
    "            data.append({\n",
    "                'video_id': f'HiDF_{label}_{folder_name}_{file_name}',\n",
    "                'label': label,\n",
    "                'path': str(file_path),\n",
    "                'manipulation': manipulation,\n",
    "                'dataset': 'HiDF',\n",
    "                'media_type': media_type\n",
    "            })\n",
    "    \n",
    "    df_result = pd.DataFrame(data)\n",
    "    print(f\"✓ HiDF: Loaded {len(df_result)} items ({data_percentage*100:.1f}%)\")\n",
    "    if len(df_result) > 0:\n",
    "        print(f\"  Real: {len(df_result[df_result['label'] == 'real'])}, Fake: {len(df_result[df_result['label'] == 'fake'])}\")\n",
    "        if 'media_type' in df_result.columns:\n",
    "            vid_count = len(df_result[df_result['media_type'] == 'vid'])\n",
    "            img_count = len(df_result[df_result['media_type'] == 'img'])\n",
    "            print(f\"  Videos: {vid_count}, Images: {img_count}\")\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2472d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD AND COMBINE ALL DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "def load_all_datasets(data_percentage=None):\n",
    "    \"\"\"\n",
    "    Load and combine all four datasets.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with combined metadata from all datasets\n",
    "    \"\"\"\n",
    "    data_percentage = data_percentage or config.DATA_PERCENTAGE\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"LOADING DATASETS ({data_percentage*100:.1f}% of each)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Load each dataset\n",
    "    df_ffpp = load_ffpp_dataset(data_percentage)\n",
    "    df_celebdf = load_celebdf_dataset(data_percentage)\n",
    "    df_dfdc = load_dfdc_dataset(data_percentage)\n",
    "    df_hidf = load_hidf_dataset(data_percentage)\n",
    "    \n",
    "    # Combine all datasets\n",
    "    all_dfs = [df for df in [df_ffpp, df_celebdf, df_dfdc, df_hidf] if len(df) > 0]\n",
    "    \n",
    "    if not all_dfs:\n",
    "        print(\"⚠ No datasets loaded!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates based on path\n",
    "    combined_df = combined_df.drop_duplicates(subset=['path'])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"COMBINED DATASET STATISTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total samples: {len(combined_df)}\")\n",
    "    print(f\"Real: {len(combined_df[combined_df['label'] == 'real'])}, Fake: {len(combined_df[combined_df['label'] == 'fake'])}\")\n",
    "    print(f\"\\nSamples per dataset:\")\n",
    "    print(combined_df['dataset'].value_counts())\n",
    "    print(f\"\\nManipulation types:\")\n",
    "    print(combined_df['manipulation'].value_counts())\n",
    "    \n",
    "    # Save combined metadata\n",
    "    combined_df.to_csv(config.INTEGRATED_METADATA_PATH, index=False)\n",
    "    print(f\"\\n✓ Metadata saved to: {config.INTEGRATED_METADATA_PATH}\")\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Load all datasets\n",
    "integrated_metadata = load_all_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f93d4",
   "metadata": {},
   "source": [
    "## Section 3: Face Detection & Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea4853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FACE EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_faces_integrated(metadata_df, output_dir, frames_per_video=20, image_size=299):\n",
    "    \"\"\"\n",
    "    Extract faces from videos/images using MTCNN.\n",
    "    Handles both video and image inputs.\n",
    "    \n",
    "    Args:\n",
    "        metadata_df: DataFrame with video/image metadata\n",
    "        output_dir: Directory to save extracted faces\n",
    "        frames_per_video: Number of frames to sample from videos\n",
    "        image_size: Output face image size\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"FACE EXTRACTION WITH MTCNN\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Initialize MTCNN\n",
    "    mtcnn = MTCNN(\n",
    "        image_size=image_size,\n",
    "        margin=40,\n",
    "        min_face_size=20,\n",
    "        thresholds=[0.6, 0.7, 0.7],\n",
    "        factor=0.709,\n",
    "        post_process=True,\n",
    "        keep_all=False,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    processed = 0\n",
    "    skipped = 0\n",
    "    errors = 0\n",
    "    \n",
    "    for idx, row in tqdm(metadata_df.iterrows(), total=len(metadata_df), desc=\"Extracting faces\"):\n",
    "        file_path = row['path']\n",
    "        video_id = row['video_id']\n",
    "        label = row['label']\n",
    "        media_type = row.get('media_type', 'vid')  # Default to video\n",
    "        \n",
    "        # Determine media type from file extension if not specified\n",
    "        ext = Path(file_path).suffix.lower()\n",
    "        if ext in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "            media_type = 'img'\n",
    "        elif ext in ['.mp4', '.avi', '.mov', '.mkv']:\n",
    "            media_type = 'vid'\n",
    "        \n",
    "        video_output_dir = output_dir / label / str(video_id)\n",
    "        \n",
    "        # Skip if already processed\n",
    "        if video_output_dir.exists():\n",
    "            existing_files = list(video_output_dir.glob('*.png'))\n",
    "            if len(existing_files) >= (1 if media_type == 'img' else frames_per_video // 2):\n",
    "                skipped += 1\n",
    "                continue\n",
    "        \n",
    "        video_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            if media_type == 'img':\n",
    "                # Handle image\n",
    "                img = cv2.imread(file_path)\n",
    "                if img is None:\n",
    "                    errors += 1\n",
    "                    continue\n",
    "                \n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                face = mtcnn(img_rgb)\n",
    "                \n",
    "                if face is not None:\n",
    "                    save_path = video_output_dir / 'frame_0.png'\n",
    "                    face_np = (face.permute(1, 2, 0).cpu().numpy() * 255.0).clip(0, 255).astype('uint8')\n",
    "                    face_bgr = cv2.cvtColor(face_np, cv2.COLOR_RGB2BGR)\n",
    "                    cv2.imwrite(str(save_path), face_bgr)\n",
    "                    processed += 1\n",
    "                else:\n",
    "                    errors += 1\n",
    "                    \n",
    "            else:\n",
    "                # Handle video\n",
    "                cap = cv2.VideoCapture(str(file_path))\n",
    "                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                \n",
    "                if total_frames == 0:\n",
    "                    errors += 1\n",
    "                    cap.release()\n",
    "                    continue\n",
    "                \n",
    "                frame_indices = torch.linspace(0, total_frames - 1, frames_per_video).long()\n",
    "                saved_count = 0\n",
    "                \n",
    "                for frame_idx in frame_indices:\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx.item())\n",
    "                    ret, frame = cap.read()\n",
    "                    \n",
    "                    if not ret:\n",
    "                        continue\n",
    "                    \n",
    "                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    face = mtcnn(frame_rgb)\n",
    "                    \n",
    "                    if face is not None:\n",
    "                        save_path = video_output_dir / f'frame_{saved_count}.png'\n",
    "                        face_np = (face.permute(1, 2, 0).cpu().numpy() * 255.0).clip(0, 255).astype('uint8')\n",
    "                        face_bgr = cv2.cvtColor(face_np, cv2.COLOR_RGB2BGR)\n",
    "                        cv2.imwrite(str(save_path), face_bgr)\n",
    "                        saved_count += 1\n",
    "                    \n",
    "                    if saved_count >= frames_per_video:\n",
    "                        break\n",
    "                \n",
    "                cap.release()\n",
    "                processed += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Face extraction complete!\")\n",
    "    print(f\"Processed: {processed}, Skipped: {skipped}, Errors: {errors}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "# Run face extraction\n",
    "extract_faces_integrated(\n",
    "    integrated_metadata, \n",
    "    config.PREPROCESSED_DIR,\n",
    "    config.FRAMES_PER_VIDEO, \n",
    "    config.FACE_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56931498",
   "metadata": {},
   "source": [
    "## Section 4: Feature Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c016339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE EXTRACTORS\n",
    "# ============================================================================\n",
    "\n",
    "class SpatialExtractor(nn.Module):\n",
    "    \"\"\"XceptionNet-based spatial feature extractor (2048-dim).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model('xception', pretrained=True, num_classes=0)\n",
    "        self.model.eval()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((299, 299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, img_path):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_tensor = self.transform(img).unsqueeze(0).to(device)\n",
    "        return self.model(img_tensor).squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "class FrequencyExtractor:\n",
    "    \"\"\"FFT-based frequency feature extractor (4-dim: mean, var, skew, kurtosis).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((299, 299)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    \n",
    "    def extract(self, img_path):\n",
    "        img = Image.open(img_path)\n",
    "        img_tensor = self.transform(img)\n",
    "        \n",
    "        # Apply 2D FFT\n",
    "        f_transform = torch.fft.fft2(img_tensor)\n",
    "        f_shift = torch.fft.fftshift(f_transform)\n",
    "        magnitude_spectrum = torch.log(torch.abs(f_shift) + 1).numpy().flatten()\n",
    "        \n",
    "        # Statistical features\n",
    "        return np.array([\n",
    "            np.mean(magnitude_spectrum),\n",
    "            np.var(magnitude_spectrum),\n",
    "            skew(magnitude_spectrum),\n",
    "            kurtosis(magnitude_spectrum)\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "\n",
    "class SemanticExtractor(nn.Module):\n",
    "    \"\"\"DINOv2 ViT-B/14 semantic feature extractor (768-dim).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14', pretrained=True)\n",
    "        self.model.eval()\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, img_path):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_tensor = self.transform(img).unsqueeze(0).to(device)\n",
    "        return self.model(img_tensor).squeeze().cpu().numpy()\n",
    "\n",
    "print(\"✓ Feature extractors defined:\")\n",
    "print(f\"  - SpatialExtractor (Xception): {config.SPATIAL_DIM}-dim\")\n",
    "print(f\"  - FrequencyExtractor (FFT): {config.FREQ_DIM}-dim\")\n",
    "print(f\"  - SemanticExtractor (DINOv2): {config.SEMANTIC_DIM}-dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d257569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXTRACT ALL FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "def extract_all_features(preprocessed_dir, output_dir):\n",
    "    \"\"\"Extract features from all three domains.\"\"\"\n",
    "    \n",
    "    # Collect all image paths\n",
    "    image_paths = []\n",
    "    for label in ['real', 'fake']:\n",
    "        label_dir = preprocessed_dir / label\n",
    "        if label_dir.exists():\n",
    "            for video_dir in label_dir.iterdir():\n",
    "                if video_dir.is_dir():\n",
    "                    for img_path in video_dir.glob('*.png'):\n",
    "                        image_paths.append(img_path)\n",
    "    \n",
    "    print(f\"\\nFound {len(image_paths)} images to process\")\n",
    "    \n",
    "    if len(image_paths) == 0:\n",
    "        print(\"⚠ No images found. Run face extraction first.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize extractors\n",
    "    spatial_extractor = SpatialExtractor().to(device)\n",
    "    freq_extractor = FrequencyExtractor()\n",
    "    semantic_extractor = SemanticExtractor().to(device)\n",
    "    \n",
    "    extractors = {\n",
    "        'spatial': (spatial_extractor, lambda e, p: e(p)),\n",
    "        'frequency': (freq_extractor, lambda e, p: e.extract(p)),\n",
    "        'semantic': (semantic_extractor, lambda e, p: e(p))\n",
    "    }\n",
    "    \n",
    "    for domain, (extractor, extract_fn) in extractors.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"EXTRACTING {domain.upper()} FEATURES\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        processed = 0\n",
    "        skipped = 0\n",
    "        \n",
    "        for img_path in tqdm(image_paths, desc=f\"Extracting {domain}\"):\n",
    "            relative_path = img_path.relative_to(preprocessed_dir)\n",
    "            feature_path = output_dir / domain / relative_path.with_suffix('.npy')\n",
    "            \n",
    "            if feature_path.exists():\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            feature_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            try:\n",
    "                features = extract_fn(extractor, img_path)\n",
    "                np.save(feature_path, features)\n",
    "                processed += 1\n",
    "            except Exception as e:\n",
    "                pass  # Silently skip errors\n",
    "        \n",
    "        print(f\"✓ {domain.capitalize()} complete: Processed={processed}, Skipped={skipped}\")\n",
    "\n",
    "# Run feature extraction\n",
    "extract_all_features(config.PREPROCESSED_DIR, config.FEATURES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddaa67f",
   "metadata": {},
   "source": [
    "## Section 5: Attention Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ATTENTION MODULES\n",
    "# ============================================================================\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation (SE) Channel Attention.\n",
    "    Learns to weight channels based on their importance.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        weights = self.fc(x)\n",
    "        return x * weights, weights\n",
    "\n",
    "\n",
    "class FrequencyBandAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention for frequency domain features.\n",
    "    Learns to weight different frequency statistics.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features=4):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(num_features, num_features * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_features * 4, num_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        weights = self.attention(x)\n",
    "        return x * weights, weights\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention module for semantic features.\n",
    "    Captures relationships within the feature vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, D = x.size()\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        x = x.unsqueeze(1)  # (B, 1, D)\n",
    "        qkv = self.qkv(x).reshape(B, 1, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        # Attention\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, 1, D)\n",
    "        out = self.proj(out).squeeze(1)\n",
    "        \n",
    "        return self.norm(x.squeeze(1) + out)  # Residual connection\n",
    "\n",
    "\n",
    "class DomainAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable attention-based fusion for multi-domain features.\n",
    "    Dynamically weights contributions from each domain.\n",
    "    \"\"\"\n",
    "    def __init__(self, spatial_dim, freq_dim, semantic_dim, fusion_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Project each domain to common space\n",
    "        self.spatial_proj = nn.Linear(spatial_dim, fusion_dim)\n",
    "        self.freq_proj = nn.Linear(freq_dim, fusion_dim)\n",
    "        self.semantic_proj = nn.Linear(semantic_dim, fusion_dim)\n",
    "        \n",
    "        # Attention network for domain weighting\n",
    "        self.domain_attention = nn.Sequential(\n",
    "            nn.Linear(fusion_dim * 3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Final fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "    \n",
    "    def forward(self, spatial_feat, freq_feat, semantic_feat):\n",
    "        # Project to common space\n",
    "        spatial_proj = self.spatial_proj(spatial_feat)\n",
    "        freq_proj = self.freq_proj(freq_feat)\n",
    "        semantic_proj = self.semantic_proj(semantic_feat)\n",
    "        \n",
    "        # Stack domains\n",
    "        domain_stack = torch.stack([spatial_proj, freq_proj, semantic_proj], dim=1)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        concat_features = torch.cat([spatial_proj, freq_proj, semantic_proj], dim=1)\n",
    "        domain_weights = self.domain_attention(concat_features)\n",
    "        \n",
    "        # Apply attention\n",
    "        weighted_features = domain_stack * domain_weights.unsqueeze(-1)\n",
    "        fused = weighted_features.sum(dim=1)\n",
    "        \n",
    "        return self.fusion(fused), domain_weights\n",
    "\n",
    "print(\"✓ Attention modules defined:\")\n",
    "print(\"  - ChannelAttention (SE) for spatial features\")\n",
    "print(\"  - FrequencyBandAttention for FFT features\")\n",
    "print(\"  - SelfAttention for semantic features\")\n",
    "print(\"  - DomainAttentionFusion for multi-domain fusion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f964f8f9",
   "metadata": {},
   "source": [
    "## Section 6: Complete Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de67953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE ATTENTION MODEL\n",
    "# ============================================================================\n",
    "\n",
    "class CompleteAttentionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete attention model with:\n",
    "    1. Individual domain attention (SE, Frequency Band, Self-Attention)\n",
    "    2. Attention-based fusion\n",
    "    \n",
    "    This is the best-performing model from the performance ranking analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spatial_dim, freq_dim, semantic_dim, hidden_dim1=512, hidden_dim2=256, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Domain-specific attention\n",
    "        self.spatial_attention = ChannelAttention(spatial_dim, reduction=16)\n",
    "        self.freq_attention = FrequencyBandAttention(freq_dim)\n",
    "        self.semantic_attention = SelfAttention(semantic_dim)\n",
    "        \n",
    "        # Fusion\n",
    "        self.fusion = DomainAttentionFusion(spatial_dim, freq_dim, semantic_dim, hidden_dim1)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim2, 1)\n",
    "        )\n",
    "        \n",
    "        # Store weights for visualization\n",
    "        self.last_domain_weights = None\n",
    "        self.last_spatial_weights = None\n",
    "        self.last_freq_weights = None\n",
    "    \n",
    "    def forward(self, spatial, freq, semantic):\n",
    "        # Apply domain-specific attention\n",
    "        spatial_attended, spatial_weights = self.spatial_attention(spatial)\n",
    "        freq_attended, freq_weights = self.freq_attention(freq)\n",
    "        semantic_attended = self.semantic_attention(semantic)\n",
    "        \n",
    "        # Store for visualization\n",
    "        self.last_spatial_weights = spatial_weights.detach()\n",
    "        self.last_freq_weights = freq_weights.detach()\n",
    "        \n",
    "        # Fusion\n",
    "        fused, domain_weights = self.fusion(spatial_attended, freq_attended, semantic_attended)\n",
    "        self.last_domain_weights = domain_weights.detach()\n",
    "        \n",
    "        return self.classifier(fused)\n",
    "\n",
    "print(\"✓ CompleteAttentionModel defined\")\n",
    "print(\"  Architecture:\")\n",
    "print(\"  1. Spatial features → Channel Attention (SE)\")\n",
    "print(\"  2. Frequency features → Frequency Band Attention\")\n",
    "print(\"  3. Semantic features → Self-Attention\")\n",
    "print(\"  4. All domains → Domain Attention Fusion\")\n",
    "print(\"  5. Fused features → Classifier (MLP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6ed1fb",
   "metadata": {},
   "source": [
    "## Section 7: Dataset Classes for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET CLASSES\n",
    "# ============================================================================\n",
    "\n",
    "class IntegratedMultiDomainDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for multi-domain features from integrated datasets.\n",
    "    Returns separate tensors per domain for the Complete Attention Model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, split='train', dataset_filter=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            split: 'train' or 'val'\n",
    "            dataset_filter: Optional - filter to specific dataset(s)\n",
    "                           e.g., 'FaceForensics++', 'Celeb-DF', 'DFDC', 'HiDF'\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        \n",
    "        # Load metadata\n",
    "        if not config.INTEGRATED_METADATA_PATH.exists():\n",
    "            raise FileNotFoundError(f\"Metadata not found: {config.INTEGRATED_METADATA_PATH}\")\n",
    "        \n",
    "        metadata_df = pd.read_csv(config.INTEGRATED_METADATA_PATH)\n",
    "        \n",
    "        # Apply dataset filter if specified\n",
    "        if dataset_filter:\n",
    "            if isinstance(dataset_filter, str):\n",
    "                metadata_df = metadata_df[metadata_df['dataset'] == dataset_filter]\n",
    "            elif isinstance(dataset_filter, list):\n",
    "                metadata_df = metadata_df[metadata_df['dataset'].isin(dataset_filter)]\n",
    "        \n",
    "        # Split data\n",
    "        train_df = metadata_df.sample(frac=config.TRAIN_RATIO, random_state=42)\n",
    "        val_df = metadata_df.drop(train_df.index)\n",
    "        df = train_df if split == 'train' else val_df\n",
    "        \n",
    "        # Collect samples\n",
    "        for _, row in df.iterrows():\n",
    "            label = row['label']\n",
    "            video_id = row['video_id']\n",
    "            dataset_name = row['dataset']\n",
    "            \n",
    "            preprocessed_dir = config.PREPROCESSED_DIR / label / video_id\n",
    "            if not preprocessed_dir.exists():\n",
    "                continue\n",
    "            \n",
    "            for frame_file in preprocessed_dir.glob('*.png'):\n",
    "                paths = {}\n",
    "                all_exist = True\n",
    "                for domain in ['spatial', 'frequency', 'semantic']:\n",
    "                    path = config.FEATURES_DIR / domain / label / video_id / frame_file.with_suffix('.npy').name\n",
    "                    if not path.exists():\n",
    "                        all_exist = False\n",
    "                        break\n",
    "                    paths[domain] = path\n",
    "                \n",
    "                if all_exist:\n",
    "                    paths['label'] = 1 if label == 'fake' else 0\n",
    "                    paths['dataset'] = dataset_name\n",
    "                    self.samples.append(paths)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        spatial = np.load(sample['spatial']).astype(np.float32)\n",
    "        frequency = np.load(sample['frequency']).astype(np.float32)\n",
    "        semantic = np.load(sample['semantic']).astype(np.float32)\n",
    "        label = np.float32(sample['label'])\n",
    "        \n",
    "        return (\n",
    "            torch.from_numpy(spatial),\n",
    "            torch.from_numpy(frequency),\n",
    "            torch.from_numpy(semantic),\n",
    "            torch.tensor(label)\n",
    "        )\n",
    "\n",
    "print(\"✓ IntegratedMultiDomainDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7d65f",
   "metadata": {},
   "source": [
    "## Section 8: Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212d88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting.\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "\n",
    "def train_complete_attention_model(dataset_filter=None, model_name='complete_attention_integrated', epochs=None):\n",
    "    \"\"\"\n",
    "    Train the Complete Attention Model on integrated datasets.\n",
    "    \n",
    "    Args:\n",
    "        dataset_filter: Optional - train on specific dataset(s)\n",
    "        model_name: Name for saving checkpoints\n",
    "        epochs: Number of epochs (None uses config default)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Training results including model, history, and metrics\n",
    "    \"\"\"\n",
    "    epochs = epochs or config.EPOCHS\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = IntegratedMultiDomainDataset(split='train', dataset_filter=dataset_filter)\n",
    "    val_dataset = IntegratedMultiDomainDataset(split='val', dataset_filter=dataset_filter)\n",
    "    \n",
    "    if len(train_dataset) == 0:\n",
    "        print(f\"⚠ No training data available for {dataset_filter or 'all datasets'}\")\n",
    "        return None\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config.BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=0, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=config.BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=0, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = CompleteAttentionModel(\n",
    "        config.SPATIAL_DIM, config.FREQ_DIM, config.SEMANTIC_DIM,\n",
    "        config.HIDDEN_DIM1, config.HIDDEN_DIM2, config.DROPOUT\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
    "    early_stopping = EarlyStopping(patience=config.PATIENCE)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [], 'val_accuracy': [],\n",
    "        'val_precision': [], 'val_recall': [], 'val_f1': [],\n",
    "        'val_auc': [], 'domain_weights': []\n",
    "    }\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    filter_str = dataset_filter if dataset_filter else 'All Datasets'\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training COMPLETE ATTENTION MODEL\")\n",
    "    print(f\"Dataset: {filter_str}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "    print(f\"Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for spatial, freq, semantic, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False):\n",
    "            spatial = spatial.to(device)\n",
    "            freq = freq.to(device)\n",
    "            semantic = semantic.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(spatial, freq, semantic)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds, all_labels, all_probs = [], [], []\n",
    "        all_domain_weights = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for spatial, freq, semantic, labels in val_loader:\n",
    "                spatial = spatial.to(device)\n",
    "                freq = freq.to(device)\n",
    "                semantic = semantic.to(device)\n",
    "                labels = labels.to(device).unsqueeze(1)\n",
    "                \n",
    "                outputs = model(spatial, freq, semantic)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "                \n",
    "                probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "                preds = probs >= 0.5\n",
    "                all_probs.extend(probs)\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(labels.cpu().numpy().flatten())\n",
    "                \n",
    "                if model.last_domain_weights is not None:\n",
    "                    all_domain_weights.append(model.last_domain_weights.cpu().numpy())\n",
    "        \n",
    "        # Metrics\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        val_prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "        val_rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "        val_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "        try:\n",
    "            val_auc = roc_auc_score(all_labels, all_probs)\n",
    "        except:\n",
    "            val_auc = 0.0\n",
    "        \n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "        history['val_loss'].append(val_loss / len(val_loader))\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        history['val_precision'].append(val_prec)\n",
    "        history['val_recall'].append(val_rec)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['val_auc'].append(val_auc)\n",
    "        \n",
    "        if all_domain_weights:\n",
    "            avg_weights = np.concatenate(all_domain_weights).mean(axis=0)\n",
    "            history['domain_weights'].append(avg_weights)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'history': history,\n",
    "                'val_acc': val_acc,\n",
    "                'config': {\n",
    "                    'data_percentage': config.DATA_PERCENTAGE,\n",
    "                    'dataset_filter': dataset_filter\n",
    "                }\n",
    "            }, config.CHECKPOINTS_DIR / f'{model_name}_best.pth')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 2 == 0 or epoch == 0:\n",
    "            print(f\"  Epoch {epoch+1}: Acc={val_acc:.4f}, F1={val_f1:.4f}, AUC={val_auc:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping(val_acc)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Final metrics\n",
    "    final_metrics = {\n",
    "        'model': model_name,\n",
    "        'dataset': filter_str,\n",
    "        'accuracy': best_val_acc,\n",
    "        'precision': history['val_precision'][-1],\n",
    "        'recall': history['val_recall'][-1],\n",
    "        'f1': history['val_f1'][-1],\n",
    "        'auc': history['val_auc'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n✓ Best Accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    return {'model': model, 'history': history, 'metrics': final_metrics}\n",
    "\n",
    "print(\"✓ Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231c292",
   "metadata": {},
   "source": [
    "## Section 9: Training on All Datasets Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN ON ALL DATASETS COMBINED\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'#'*70}\")\n",
    "print(\"# TRAINING ON ALL INTEGRATED DATASETS\")\n",
    "print(f\"{'#'*70}\")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "# Train on all datasets combined\n",
    "result_all = train_complete_attention_model(\n",
    "    dataset_filter=None,\n",
    "    model_name='complete_attention_all_datasets'\n",
    ")\n",
    "\n",
    "if result_all:\n",
    "    all_results['all_datasets'] = result_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacce443",
   "metadata": {},
   "source": [
    "## Section 10: Per-Dataset Training & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa7686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAIN ON EACH DATASET INDIVIDUALLY\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'#'*70}\")\n",
    "print(\"# TRAINING ON INDIVIDUAL DATASETS\")\n",
    "print(f\"{'#'*70}\")\n",
    "\n",
    "datasets = ['FaceForensics++', 'Celeb-DF', 'DFDC', 'HiDF']\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training on {dataset_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    result = train_complete_attention_model(\n",
    "        dataset_filter=dataset_name,\n",
    "        model_name=f'complete_attention_{dataset_name.replace(\"+\", \"pp\").replace(\"-\", \"_\")}'\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        all_results[dataset_name] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3c4ffa",
   "metadata": {},
   "source": [
    "## Section 11: Results Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee1d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RESULTS SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING RESULTS SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if all_results:\n",
    "    # Create results DataFrame\n",
    "    results_data = []\n",
    "    for name, result in all_results.items():\n",
    "        if result and 'metrics' in result:\n",
    "            metrics = result['metrics']\n",
    "            results_data.append({\n",
    "                'Dataset': name,\n",
    "                'Accuracy': metrics.get('accuracy', 0),\n",
    "                'Precision': metrics.get('precision', 0),\n",
    "                'Recall': metrics.get('recall', 0),\n",
    "                'F1-Score': metrics.get('f1', 0),\n",
    "                'AUC': metrics.get('auc', 0)\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    results_df = results_df.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    print(\"\\n📊 Performance Comparison:\")\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(config.RESULTS_DIR / 'integrated_training_results.csv', index=False)\n",
    "    print(f\"\\n✓ Results saved to: {config.RESULTS_DIR / 'integrated_training_results.csv'}\")\n",
    "else:\n",
    "    print(\"No results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcad4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "if all_results and len(results_data) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Accuracy comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(results_df)))\n",
    "    bars = ax1.bar(results_df['Dataset'], results_df['Accuracy'], color=colors)\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Accuracy by Dataset')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    for bar, acc in zip(bars, results_df['Accuracy']):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                 f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Plot 2: F1-Score comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    bars = ax2.bar(results_df['Dataset'], results_df['F1-Score'], color=colors)\n",
    "    ax2.set_ylabel('F1-Score')\n",
    "    ax2.set_title('F1-Score by Dataset')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    for bar, f1 in zip(bars, results_df['F1-Score']):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                 f'{f1:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Plot 3: All metrics comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    x = np.arange(len(results_df))\n",
    "    width = 0.2\n",
    "    ax3.bar(x - 1.5*width, results_df['Accuracy'], width, label='Accuracy')\n",
    "    ax3.bar(x - 0.5*width, results_df['Precision'], width, label='Precision')\n",
    "    ax3.bar(x + 0.5*width, results_df['Recall'], width, label='Recall')\n",
    "    ax3.bar(x + 1.5*width, results_df['F1-Score'], width, label='F1-Score')\n",
    "    ax3.set_ylabel('Score')\n",
    "    ax3.set_title('All Metrics Comparison')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(results_df['Dataset'], rotation=45, ha='right')\n",
    "    ax3.legend(loc='lower right')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    \n",
    "    # Plot 4: Training curves (if available)\n",
    "    ax4 = axes[1, 1]\n",
    "    for name, result in all_results.items():\n",
    "        if result and 'history' in result:\n",
    "            history = result['history']\n",
    "            if 'val_accuracy' in history:\n",
    "                ax4.plot(history['val_accuracy'], label=name, marker='o', markersize=3)\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Validation Accuracy')\n",
    "    ax4.set_title('Training Curves')\n",
    "    ax4.legend(loc='lower right', fontsize=8)\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.RESULTS_DIR / 'integrated_training_comparison.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ Visualization saved to: {config.RESULTS_DIR / 'integrated_training_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be45e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOMAIN ATTENTION WEIGHTS ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DOMAIN ATTENTION WEIGHTS ANALYSIS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "domain_labels = ['Spatial', 'Frequency', 'Semantic']\n",
    "\n",
    "if all_results:\n",
    "    for name, result in all_results.items():\n",
    "        if result and 'history' in result:\n",
    "            domain_weights = result['history'].get('domain_weights', [])\n",
    "            if domain_weights:\n",
    "                final_weights = domain_weights[-1]\n",
    "                print(f\"\\n{name}:\")\n",
    "                for label, weight in zip(domain_labels, final_weights):\n",
    "                    print(f\"  {label}: {weight:.4f} ({weight*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22814a0",
   "metadata": {},
   "source": [
    "## Section 12: Cross-Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f1966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CROSS-DATASET EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "def evaluate_cross_dataset(model, source_dataset, target_dataset):\n",
    "    \"\"\"\n",
    "    Evaluate a model trained on source_dataset on target_dataset.\n",
    "    Tests generalization across different deepfake datasets.\n",
    "    \"\"\"\n",
    "    # Create target dataset\n",
    "    target_data = IntegratedMultiDomainDataset(split='val', dataset_filter=target_dataset)\n",
    "    \n",
    "    if len(target_data) == 0:\n",
    "        return None\n",
    "    \n",
    "    target_loader = DataLoader(target_data, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for spatial, freq, semantic, labels in target_loader:\n",
    "            spatial = spatial.to(device)\n",
    "            freq = freq.to(device)\n",
    "            semantic = semantic.to(device)\n",
    "            \n",
    "            outputs = model(spatial, freq, semantic)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy().flatten() >= 0.5\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'f1': f1_score(all_labels, all_preds, zero_division=0)\n",
    "    }\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CROSS-DATASET GENERALIZATION ANALYSIS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "datasets = ['FaceForensics++', 'Celeb-DF', 'DFDC', 'HiDF']\n",
    "cross_results = {}\n",
    "\n",
    "# Evaluate model trained on all datasets on each individual dataset\n",
    "if 'all_datasets' in all_results and all_results['all_datasets']:\n",
    "    model_all = all_results['all_datasets']['model']\n",
    "    \n",
    "    print(\"\\nModel trained on ALL datasets, evaluated on:\")\n",
    "    for target in datasets:\n",
    "        result = evaluate_cross_dataset(model_all, 'all', target)\n",
    "        if result:\n",
    "            cross_results[f'all->{target}'] = result\n",
    "            print(f\"  {target}: Acc={result['accuracy']:.4f}, F1={result['f1']:.4f}\")\n",
    "\n",
    "# Cross-dataset evaluation for each individual model\n",
    "print(\"\\nCross-dataset evaluation (train on A, test on B):\")\n",
    "for source in datasets:\n",
    "    if source in all_results and all_results[source]:\n",
    "        model = all_results[source]['model']\n",
    "        for target in datasets:\n",
    "            if target != source:\n",
    "                result = evaluate_cross_dataset(model, source, target)\n",
    "                if result:\n",
    "                    cross_results[f'{source}->{target}'] = result\n",
    "                    print(f\"  {source} -> {target}: Acc={result['accuracy']:.4f}, F1={result['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2750419",
   "metadata": {},
   "source": [
    "## Section 13: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78233d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'#'*70}\")\n",
    "print(\"# INTEGRATED TRAINING COMPLETE - FINAL SUMMARY\")\n",
    "print(f\"{'#'*70}\")\n",
    "\n",
    "print(f\"\\n📊 Configuration:\")\n",
    "print(f\"  - Data Percentage: {config.DATA_PERCENTAGE * 100:.1f}%\")\n",
    "print(f\"  - Model: Complete Attention Model\")\n",
    "print(f\"  - Feature Dimensions: Spatial({config.SPATIAL_DIM}), Freq({config.FREQ_DIM}), Semantic({config.SEMANTIC_DIM})\")\n",
    "\n",
    "print(f\"\\n📈 Best Results:\")\n",
    "if all_results:\n",
    "    best_result = max(all_results.items(), key=lambda x: x[1]['metrics']['accuracy'] if x[1] else 0)\n",
    "    print(f\"  - Best Dataset: {best_result[0]}\")\n",
    "    print(f\"  - Best Accuracy: {best_result[1]['metrics']['accuracy']:.4f}\")\n",
    "    print(f\"  - Best F1-Score: {best_result[1]['metrics']['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\n📁 Saved Files:\")\n",
    "print(f\"  - Metadata: {config.INTEGRATED_METADATA_PATH}\")\n",
    "print(f\"  - Results: {config.RESULTS_DIR / 'integrated_training_results.csv'}\")\n",
    "print(f\"  - Checkpoints: {config.CHECKPOINTS_DIR}\")\n",
    "\n",
    "print(f\"\\n{'#'*70}\")\n",
    "print(\"# TRAINING PIPELINE COMPLETE\")\n",
    "print(f\"{'#'*70}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
